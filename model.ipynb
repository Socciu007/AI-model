{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['context', 'question', 'answer', 'distractors']\n",
      "['Unnamed: 0', 'question', 'answer', 'answer_context']\n"
     ]
    }
   ],
   "source": [
    "#load data from huggingface\n",
    "from datasets import load_dataset\n",
    "ds_train = load_dataset(\"mou3az/Question-Answering-Generation-Choices\")\n",
    "ds_val = load_dataset(\"xwjzds/extractive_qa_question_answering_hr\")\n",
    "print(ds_train[\"train\"].column_names)\n",
    "print(ds_val[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data into local drive\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df_train = pd.DataFrame(ds_train[\"train\"])[['question', 'answer', 'context']]\n",
    "df_val = pd.DataFrame(ds_val[\"train\"])[['question', 'answer', 'answer_context']]\n",
    "\n",
    "# rename columns\n",
    "df_val.rename(columns={'answer_context': 'context'}, inplace=True)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Adjust file paths with the correct directory structure\n",
    "train_file = os.path.join(data_dir, 'train_set.xlsx')\n",
    "val_file = os.path.join(data_dir, 'val_set.xlsx')\n",
    "\n",
    "# Save data to an Excel file\n",
    "df_train.to_excel(train_file, index=False)\n",
    "df_val.to_excel(val_file, index=False)\n",
    "\n",
    "print(f\"The data has been saved to the file {train_file}\")\n",
    "print(f\"The data has been saved to the file {val_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from Excel files\n",
    "train = pd.read_excel('data/train_set.xlsx')\n",
    "val = pd.read_excel('data/val_set.xlsx')\n",
    "\n",
    "# Remove rows with missing values\n",
    "train_cleaned = train.dropna()\n",
    "val_cleaned = val.dropna()\n",
    "\n",
    "# Replace specific values in 'context' column\n",
    "val_filled = val_cleaned.replace({\"context\": {\"Employee: \": \"\"}}, regex=True)\n",
    "\n",
    "# Remove duplicate records\n",
    "train_deduplicated = train_cleaned.drop_duplicates()\n",
    "val_deduplicated = val_filled.drop_duplicates()\n",
    "\n",
    "# Convert data types if necessary on cleaned and deduplicated dataframes\n",
    "train_dt = train_deduplicated.copy()\n",
    "train_dt.loc[:, 'question'] = train_dt['question'].astype(str)\n",
    "train_dt.loc[:, 'answer'] = train_dt['answer'].astype(str)\n",
    "train_dt.loc[:, 'context'] = train_dt['context'].astype(str)\n",
    "\n",
    "val_dt = val_deduplicated.copy()\n",
    "val_dt.loc[:, 'question'] = val_dt['question'].astype(str)\n",
    "val_dt.loc[:, 'answer'] = val_dt['answer'].astype(str)\n",
    "val_dt.loc[:, 'context'] = val_deduplicated['context'].astype(str)\n",
    "\n",
    "# Display the first 5 rows after cleaning\n",
    "train_text= list(train_deduplicated['question'])\n",
    "val_text= list(val_deduplicated['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\test\\chatbot\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Administrator\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_huggingface_token' with your actual Hugging Face token\n",
    "huggingface_token = \"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\"\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 20.35it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Load the tokenizer and the pre-trained Llama3 model\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Initialize the model without specifying the device here\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\",)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], padding=\"max_length\", truncation=True, max_length=258, return_tensors=\"pt\")\n",
    "\n",
    "# Apply the tokenization\n",
    "train_data_tokenized = train_dt.apply(tokenize_function, axis=1)\n",
    "val_data_tokenized = val_dt.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Define the custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, answers):\n",
    "        self.input_ids = [data['input_ids'][0] for data in tokenized_data]\n",
    "        self.attention_mask = [data['attention_mask'][0] for data in tokenized_data]\n",
    "        self.labels = tokenizer(answers, padding=\"max_length\", truncation=True, max_length=258, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Prepare the dataset training\n",
    "dataset_train = CustomDataset(train_data_tokenized, train_dt['answer'].tolist())\n",
    "dataset_vali = CustomDataset(val_data_tokenized, val_dt['answer'].tolist())\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"my-lln-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "# pass \"wandb\" to the 'report_to' parameter to turn on wandb logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='models',\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    max_steps = 100,\n",
    "    save_steps = 100\n",
    ")\n",
    "\n",
    "# define the trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_vali,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1 + 1 =\n",
      "Response: 1 + 1 = 2 (1 + 1 = 2)\n",
      "2 + 2 = 4 (2 + 2 = 4)\n",
      "3 + 3 = 6 (3 + 3 = 6)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Function to generate response\n",
    "def generate_response(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "question = \"1 + 1 =\"\n",
    "response = generate_response(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

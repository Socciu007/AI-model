{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in .\\env\\lib\\site-packages (0.17.4)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in .\\env\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in .\\env\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in .\\env\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in .\\env\\lib\\site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in .\\env\\lib\\site-packages (from wandb) (5.27.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in .\\env\\lib\\site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in .\\env\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in .\\env\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in .\\env\\lib\\site-packages (from wandb) (2.8.0)\n",
      "Requirement already satisfied: setproctitle in .\\env\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in .\\env\\lib\\site-packages (from wandb) (70.2.0)\n",
      "Requirement already satisfied: colorama in .\\env\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in .\\env\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in .\\env\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in .\\env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator\\_netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 10713c140f304d47b99c544f0f74e73b7d667cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\test\\chatbot\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5426.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# This script needs these libraries to be installed:\n",
    "#   numpy, transformers, datasets\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "# download prepare the data\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(300))\n",
    "\n",
    "small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "small_eval_dataset = small_train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "f:\\test\\chatbot\\env\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanhtien310701\u001b[0m (\u001b[33mmanhtien310701-fanyuan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\test\\chatbot\\wandb\\run-20240710_094227-vd07t9u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/vd07t9u5' target=\"_blank\">models</a></strong> to <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/vd07t9u5' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/vd07t9u5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [02:35<48:31, 30.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.606, 'grad_norm': 1.8802423477172852, 'learning_rate': 4.75e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [05:11<46:01, 30.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5961, 'grad_norm': 1.2873133420944214, 'learning_rate': 4.5e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [07:44<43:53, 30.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5328, 'grad_norm': 1.729570984840393, 'learning_rate': 4.25e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [10:31<43:09, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4426, 'grad_norm': 3.3474764823913574, 'learning_rate': 4e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 20/100 [15:56<43:09, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3307403326034546, 'eval_accuracy': 0.443, 'eval_runtime': 324.6889, 'eval_samples_per_second': 3.08, 'eval_steps_per_second': 0.099, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [18:30<1:07:48, 54.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3119, 'grad_norm': 3.390974283218384, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [21:10<42:03, 36.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2908, 'grad_norm': 6.429003715515137, 'learning_rate': 3.5e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [23:28<33:08, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.133, 'grad_norm': 2.993131160736084, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [26:07<32:26, 32.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.098, 'grad_norm': 2.783486843109131, 'learning_rate': 3e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 40/100 [31:36<32:26, 32.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.02614426612854, 'eval_accuracy': 0.61, 'eval_runtime': 328.49, 'eval_samples_per_second': 3.044, 'eval_steps_per_second': 0.097, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [34:12<50:41, 55.29s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0678, 'grad_norm': 4.699380874633789, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [36:46<29:07, 34.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0769, 'grad_norm': 4.055044174194336, 'learning_rate': 2.5e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [39:20<23:31, 31.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9892, 'grad_norm': 4.091495990753174, 'learning_rate': 2.25e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [41:59<21:11, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9387, 'grad_norm': 4.172548770904541, 'learning_rate': 2e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 60%|██████    | 60/100 [47:28<21:11, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8711727261543274, 'eval_accuracy': 0.677, 'eval_runtime': 329.0638, 'eval_samples_per_second': 3.039, 'eval_steps_per_second': 0.097, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [49:47<29:29, 50.56s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8702, 'grad_norm': 6.584935665130615, 'learning_rate': 1.75e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [52:23<17:02, 34.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9068, 'grad_norm': 6.422648906707764, 'learning_rate': 1.5e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [55:06<13:37, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.881, 'grad_norm': 6.309064865112305, 'learning_rate': 1.25e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [57:39<10:13, 30.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8477, 'grad_norm': 4.51299524307251, 'learning_rate': 1e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 80%|████████  | 80/100 [1:02:55<10:13, 30.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7742425203323364, 'eval_accuracy': 0.737, 'eval_runtime': 315.506, 'eval_samples_per_second': 3.17, 'eval_steps_per_second': 0.101, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [1:05:20<13:02, 52.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7879, 'grad_norm': 3.375075101852417, 'learning_rate': 7.5e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [1:07:51<05:44, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8067, 'grad_norm': 3.0325076580047607, 'learning_rate': 5e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [1:10:26<02:34, 30.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8513, 'grad_norm': 6.684648036956787, 'learning_rate': 2.5e-06, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:12:40<00:00, 29.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7287, 'grad_norm': 3.5257890224456787, 'learning_rate': 0.0, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 100/100 [1:18:03<00:00, 29.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7370273470878601, 'eval_accuracy': 0.743, 'eval_runtime': 322.664, 'eval_samples_per_second': 3.099, 'eval_steps_per_second': 0.099, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:18:04<00:00, 29.64s/it]max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4690.3566, 'train_samples_per_second': 0.682, 'train_steps_per_second': 0.021, 'train_loss': 1.0882170462608338, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:18:05<00:00, 46.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▆██</td></tr><tr><td>eval/loss</td><td>█▄▃▁▁</td></tr><tr><td>eval/runtime</td><td>▆██▁▅</td></tr><tr><td>eval/samples_per_second</td><td>▃▁▁█▄</td></tr><tr><td>eval/steps_per_second</td><td>▄▁▁█▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▁▂▄▄█▃▃▅▅▅▅███▅▄▃█▄</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▅▄▄▄▄▃▃▂▂▂▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.743</td></tr><tr><td>eval/loss</td><td>0.73703</td></tr><tr><td>eval/runtime</td><td>322.664</td></tr><tr><td>eval/samples_per_second</td><td>3.099</td></tr><tr><td>eval/steps_per_second</td><td>0.099</td></tr><tr><td>total_flos</td><td>414380191457280.0</td></tr><tr><td>train/epoch</td><td>3.125</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>3.52579</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7287</td></tr><tr><td>train_loss</td><td>1.08822</td></tr><tr><td>train_runtime</td><td>4690.3566</td></tr><tr><td>train_samples_per_second</td><td>0.682</td></tr><tr><td>train_steps_per_second</td><td>0.021</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">models</strong> at: <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/vd07t9u5' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/vd07t9u5</a><br/> View project at: <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240710_094227-vd07t9u5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"my-model-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "# pass \"wandb\" to the 'report_to' parameter to turn on wandb logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='models',\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    max_steps = 100,\n",
    "    save_steps = 100\n",
    ")\n",
    "\n",
    "# define the trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:apbod8mt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-deluge-8</strong> at: <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/apbod8mt' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/apbod8mt</a><br/> View project at: <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240710_130345-apbod8mt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:apbod8mt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\test\\chatbot\\wandb\\run-20240710_130417-6ap0wzwd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/6ap0wzwd' target=\"_blank\">sunny-sun-9</a></strong> to <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/6ap0wzwd' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/my-model-project/runs/6ap0wzwd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-vd07t9u5:v1, 255.44MB. 4 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 3\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize Wandb run\n",
    "run = wandb.init()\n",
    "\n",
    "# Sử dụng artifact để tải mô hình\n",
    "artifact = run.use_artifact('manhtien310701-fanyuan/my-model-project/model-vd07t9u5:v1', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load mô hình từ thư mục đã tải xuống\n",
    "model = DistilBertForSequenceClassification.from_pretrained(artifact_dir)\n",
    "\n",
    "# Chuẩn bị dữ liệu đầu vào\n",
    "input_text = \"Good fish sandwich.\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Dự đoán với mô hình\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# In kết quả dự đoán\n",
    "print(f\"Predicted label: {predictions.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\test\\chatbot\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "from time import time\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer,setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Administrator\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Administrator\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\test\\chatbot\\wandb\\run-20240722_140957-26afrxy3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manhtien310701-fanyuan/Fine-tune%20Llama%203%208B/runs/26afrxy3' target=\"_blank\">fearless-lake-2</a></strong> to <a href='https://wandb.ai/manhtien310701-fanyuan/Fine-tune%20Llama%203%208B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manhtien310701-fanyuan/Fine-tune%20Llama%203%208B' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/Fine-tune%20Llama%203%208B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manhtien310701-fanyuan/Fine-tune%20Llama%203%208B/runs/26afrxy3' target=\"_blank\">https://wandb.ai/manhtien310701-fanyuan/Fine-tune%20Llama%203%208B/runs/26afrxy3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import wandb\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "huggingface_hub_token = \"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\"\n",
    "wandb_api = \"10713c140f304d47b99c544f0f74e73b7d667cbf\"\n",
    "\n",
    "login(token = huggingface_hub_token)\n",
    "\n",
    "wandb.login(key=wandb_api)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3 8B', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model {model:llama-3, framework: transformers, size: 8B, type: 8b-chat-hf, version: 1}\n",
    "# model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config some parameters\n",
    "num_of_epochs = 1\n",
    "\n",
    "# No change params\n",
    "use_4bit, bnb_4bit_compute_dtype, bnb_4bit_quant_type, use_nested_quant = True, \"float16\", \"nf4\", False # To quantization\n",
    "lora_r, lora_alpha, lora_dropout = 64, 16, 0.1\n",
    "fp16, bf16 = False, False\n",
    "per_device_train_batch_size, per_device_eval_batch_size = 4, 4\n",
    "gradient_accumulation_steps, gradient_checkpointing, max_grad_norm = 1, True, 0.3\n",
    "learning_rate, weight_decay, optim = 2e-4, 0.001, \"paged_adamw_32bit\"\n",
    "lr_scheduler_type, max_steps, warmup_ratio = \"cosine\", -1, 0.03\n",
    "group_by_length, save_steps, logging_steps = True, 0, 25\n",
    "max_seq_length, packing, device_map = None, False, {\"\": \"cuda:0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config quantization model\n",
    "compute_dtype = torch.bfloat16\n",
    "attn_implementation = \"eager\"\n",
    "# config QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype, #\n",
    "    bnb_4bit_use_double_quant=True # use_nested_quant\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-model and load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time()\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_size = \"right\"\n",
    "\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "ds = load_dataset(\"ruslanmv/ai-medical-chatbot\", split=\"all\")\n",
    "ds = ds.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "ds = ds.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "ds['text'][3]\n",
    "\n",
    "ds = ds.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config: set the rank for LoRA to 4, to reduce the number of trainable parameters.\n",
    "peft_config = LoraConfig(\n",
    "    r=16, #lora_r=64\n",
    "    lora_alpha=32, #lora_alpha=16\n",
    "    lora_dropout=0.05, #0.1\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results_llama3_sft/\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    optim=optim,\n",
    "    per_device_train_batch_size=1, #4\n",
    "    gradient_accumulation_steps=2, #1\n",
    "    per_device_eval_batch_size=1, #4\n",
    "    # save_steps=1, #0\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1, #25\n",
    "    learning_rate=learning_rate, #2e-4\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    eval_steps=0.2,\n",
    "    # max_steps=20, #-1\n",
    "    num_train_epochs=num_of_epochs, #1\n",
    "    warmup_steps=10,\n",
    "    group_by_length=group_by_length,\n",
    "    report_to=\"wandb\"\n",
    "    # lr_scheduler_type=\"linear\", #\"cosine\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
